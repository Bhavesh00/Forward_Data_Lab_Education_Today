- Input a list of (name, institution).

- Crawler: Collect data -- Rohan,Bhavesh
    - Go to Google. Search (name,institution). Filter results --> URLs that are faculty homepages.

    - Go through each page. Extract information: Biography, Education, Awards, Research interests, Publications
    
    - Build a console, allow for human operator to validate/correct data 

- Data Mining -- Chen, Haoxiang
    - Generate keywords for each professor (from publications). 
    - Link them in a graph (by matching keywords).
- Search system -- Together
    - Allow users to search professors by keywords
    - Return a rank list of professors
    - When user clicks on a professor, it shows a result page with details of this professor.

- Two Weeks -- to have a complete end-to-end, with some subroutines hard-coded. Interface/function should be complete. (-July 2nd-)   





INFO FROM Edu_today 2 (Previous implementation)

get_homepage_info Folder:

corpus: training dataset for RandomForest Classifier

data_process.py: Run with no argument and add some URLs to the url_list to test. It get and parse the HTML file to get text content and then doing some data cleaning on raw_data and finally return a list of strings which can be append to training data or input to classifier.

random_forest.py: Run with no argument and it will train a RandomForest model with training data from corpus file and extract feature using TfidfVectorizer. After training is done, the classifier model is saved to pickle file called text_classifier and feature model is saved to pickle file called vectorizer using pickle and will be used doing prediction.

predict.py: Run with no argument and add some URLs to the url_list to test the result. It will loads the classifier and vectorizer model from text_classifier and vectorizer and print out the prediction result.

text_classifier: RandomForest classifier model

vectorizer: feature vectorizer model
